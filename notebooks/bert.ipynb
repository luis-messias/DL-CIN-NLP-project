{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratting prediction using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, example=\"train\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        if example == \"train\":\n",
    "            dataset = pd.read_csv('../Kaggle-dataset/pre-processed/trainDataset.csv',encoding=\"latin1\")\n",
    "        elif example == \"val\":\n",
    "            dataset = pd.read_csv('../Kaggle-dataset/pre-processed/valDataset.csv',encoding=\"latin1\")\n",
    "        else:\n",
    "            dataset = pd.read_csv('../Kaggle-dataset/pre-processed/testDataset.csv',encoding=\"latin1\")\n",
    "\n",
    "        self.text_data = dataset[\"textFull\"]\n",
    "        self.tokens = self.tokenizer(list(dataset[\"textFull\"]), padding = True, truncation=True)\n",
    "        self.labels = dataset[\"rating\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        for k, v in self.tokens.items():\n",
    "            item[k] = torch.tensor(v[idx]).to(device)\n",
    "        item['labels'] = torch.tensor(self.labels[idx]).to(device) - 1\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = MyDataSet(example = \"train\")\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_dataset = MyDataSet(example = \"val\")\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_dataset = MyDataSet(example = \"test\")\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=5).to(device)\n",
    "bert.bert.requires_grad_(False)\n",
    "bert.classifier.requires_grad_(True)\n",
    "optimizer = torch.optim.AdamW(bert.parameters(), lr=1e-4) \n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 ----------------------------------\n",
      "1700  : 0 : 1.7215142250061035 100 : 131.0129935145378 200 : 255.11851334571838 300 : 379.512457549572 400 : 503.1920762062073 500 : 628.201465010643 600 : 749.459127664566 700 : 871.3758177757263 800 : 989.6556088328362 900 : 1107.2049055099487 1000 : 1228.9010525941849 1100 : 1350.494466483593 1200 : 1472.497771203518 1300 : 1593.3632286787033 1400 : 1711.6744503974915 1500 : 1831.2689664959908 1600 : 1949.5232699513435 \n",
      "Train loss: 2064.500599563122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [20:59<1:02:57, 1259.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  299.08642315864563\n",
      "Val Acc:  0.5558960529541555\n",
      "Epoch:  2 ----------------------------------\n",
      "1700  : 0 : 1.1871343851089478 100 : 119.28984063863754 200 : 235.25229382514954 300 : 351.8604729771614 400 : 469.61310613155365 500 : 589.9338834881783 600 : 707.850201010704 700 : 826.0017397403717 800 : 940.9992127418518 900 : 1055.1705548763275 1000 : 1172.9520829319954 1100 : 1286.08813560009 1200 : 1401.047224342823 1300 : 1517.2572385668755 1400 : 1633.3195210695267 1500 : 1749.3205134868622 1600 : 1859.8151040673256 \n",
      "Train loss: 1973.696773469448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [42:15<42:17, 1268.98s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  289.37465238571167\n",
      "Val Acc:  0.5644765873988723\n",
      "Unfreezing layers\n",
      "Epoch:  3 ----------------------------------\n",
      "1700  : 0 : 1.2591253519058228 100 : 96.14495965838432 200 : 178.54502138495445 300 : 253.49826389551163 400 : 328.7114703953266 500 : 403.01564890146255 600 : 474.2522301375866 700 : 539.0294333994389 800 : 609.4439779222012 900 : 679.3897665143013 1000 : 745.7661032378674 1100 : 815.6719637215137 1200 : 883.5239301025867 1300 : 951.8685435950756 1400 : 1018.9645284116268 1500 : 1084.6997320353985 1600 : 1151.3484317660332 \n",
      "Train loss: 1216.8886517584324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [1:37:47<36:51, 2211.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  162.5352668762207\n",
      "Val Acc:  0.7632998283893111\n",
      "Unfreezing layers\n",
      "Epoch:  4 ----------------------------------\n",
      "1700  : 0 : 0.5634540319442749 100 : 60.730956330895424 200 : 119.76999552547932 300 : 180.47433419525623 400 : 242.06087498366833 500 : 301.8812962025404 600 : 360.6072488874197 700 : 422.6696712821722 800 : 482.9623290747404 900 : 540.7539758831263 1000 : 600.9427126795053 1100 : 660.946772262454 1200 : 721.4128616452217 1300 : 781.1115184724331 1400 : 840.459117859602 1500 : 901.9782637059689 1600 : 960.2429176270962 \n",
      "Train loss: 1017.5006348788738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [2:32:56<00:00, 2294.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  158.40258176624775\n",
      "Val Acc:  0.7696739396911008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "num_epochs = 4\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    if epoch >= 2:\n",
    "        print(\"Unfreezing layers\")\n",
    "        optimizer = torch.optim.AdamW(bert.parameters(), lr=1e-5)\n",
    "        bert.bert.requires_grad_(True)\n",
    "        bert.classifier.requires_grad_(True)\n",
    "        \n",
    "    print(\"Epoch: \",(epoch + 1), \"----------------------------------\")\n",
    "    bert.train()\n",
    "    total_loss_train = 0\n",
    "    print(len((train_loader)),\" :\", end=\" \")\n",
    "\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = bert(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "        \n",
    "        pred = outputs.logits\n",
    "        loss = loss_fn(pred, batch['labels'])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_train += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print(i, \":\", total_loss_train, end=\" \")\n",
    "    print(f\"\\nTrain loss: {total_loss_train}\")\n",
    "\n",
    "    bert.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss_val = 0\n",
    "    for i, batch in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            outputs = bert(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fn(logits, batch['labels'])\n",
    "        total_loss_val += loss.item()\n",
    "        \n",
    "        correct += (logits.argmax(1) == batch['labels']).sum().item()\n",
    "        total += (logits.argmax(1) == logits.argmax(1)).sum().item()\n",
    "    print(f\"Val loss: \",total_loss_val)\n",
    "    print(f\"Val Acc: \",correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('bert/bert.pkl','wb') as f:\n",
    "    pickle.dump(bert,f)\n",
    "with open('bert/tokenizer.pkl','wb') as f:\n",
    "    pickle.dump(AutoTokenizer.from_pretrained(\"bert-base-cased\"),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "255it [02:42,  1.57it/s]\n",
      "170it [01:47,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "def predict(bert, loader):\n",
    "    Y = []\n",
    "    Y_pred = []\n",
    "    for i, batch in tqdm(enumerate(loader)):\n",
    "        with torch.no_grad():\n",
    "            Y_pred += bert.to(device)(input_ids = batch['input_ids'], attention_mask = batch['attention_mask']).logits.argmax(1).to('cpu')\n",
    "            Y += batch['labels'].to('cpu')\n",
    "    return Y,Y_pred\n",
    "\n",
    "\n",
    "valDataSet = pd.read_csv('../Kaggle-dataset/pre-processed/valDataset.csv',encoding=\"latin1\")\n",
    "testDataSet = pd.read_csv('../Kaggle-dataset/pre-processed/testDataset.csv',encoding=\"latin1\")\n",
    "            \n",
    "Y_val, Y_val_pred = predict(bert, val_loader)\n",
    "Y_test, Y_test_pred = predict(bert, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val\n",
      "\tAcuracia:  0.7696739396911008\n",
      "\tF1:  [0.79933481 0.275      0.42929293 0.40878828 0.89294093]\n",
      "Test\n",
      "\tAcuracia:  0.7676470588235295\n",
      "\tF1:  [0.77685226 0.23369565 0.40897098 0.43112701 0.89724626]\n"
     ]
    }
   ],
   "source": [
    "acc_val = sklearn.metrics.accuracy_score(Y_val, Y_val_pred)\n",
    "f1_val = sklearn.metrics.f1_score(Y_val, Y_val_pred, average=None)\n",
    "acc_test = sklearn.metrics.accuracy_score(Y_test, Y_test_pred)\n",
    "f1_test = sklearn.metrics.f1_score(Y_test, Y_test_pred, average=None)\n",
    "\n",
    "print(\"Val\")\n",
    "print(\"\\tAcuracia: \", acc_val)\n",
    "print(\"\\tF1: \", f1_val)\n",
    "\n",
    "print(\"Test\")\n",
    "print(\"\\tAcuracia: \", acc_test)\n",
    "print(\"\\tF1: \", f1_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npl-esp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
