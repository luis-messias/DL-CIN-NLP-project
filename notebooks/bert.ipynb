{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratting prediction using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, example=\"train\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        if example == \"train\":\n",
    "            dataset = pd.read_csv('../Kaggle-dataset/pre-processed/trainDataset.csv',encoding=\"latin1\")\n",
    "        elif example == \"val\":\n",
    "            dataset = pd.read_csv('../Kaggle-dataset/pre-processed/valDataset.csv',encoding=\"latin1\")\n",
    "        else:\n",
    "            dataset = pd.read_csv('../Kaggle-dataset/pre-processed/testDataset.csv',encoding=\"latin1\")\n",
    "\n",
    "        self.text_data = dataset[\"textFull\"]\n",
    "        self.tokens = self.tokenizer(list(dataset[\"textFull\"]), padding = True, truncation=True)\n",
    "        self.labels = dataset[\"rating\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        for k, v in self.tokens.items():\n",
    "            item[k] = torch.tensor(v[idx]).to(device)\n",
    "        item['labels'] = torch.tensor(self.labels[idx]).to(device) - 1\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = MyDataSet(example = \"train\")\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_dataset = MyDataSet(example = \"val\")\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_dataset = MyDataSet(example = \"test\")\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=5).to(device)\n",
    "bert.bert.requires_grad_(False)\n",
    "bert.classifier.requires_grad_(True)\n",
    "optimizer = torch.optim.AdamW(bert.parameters(), lr=1e-4) \n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 ----------------------------------\n",
      "1700  : 0 : 1.8335634469985962 100 : 132.05991274118423 200 : 256.4970574975014 300 : 381.1644026041031 400 : 503.49818456172943 500 : 627.6555376648903 600 : 748.6356809735298 700 : 869.3627625107765 800 : 991.6781249046326 900 : 1114.1847217082977 1000 : 1235.51602435112 1100 : 1355.6421993374825 1200 : 1474.2077613472939 1300 : 1591.0556537508965 1400 : 1708.5779254436493 1500 : 1828.6925369501114 1600 : 1944.4770182967186 \n",
      "Train loss: 2063.0047773718834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [21:06<1:03:20, 1266.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  298.36660146713257\n",
      "Val Acc:  0.5505025741603334\n",
      "Epoch:  2 ----------------------------------\n",
      "1700  : 0 : 1.2560410499572754 100 : 115.96689856052399 200 : 232.16897064447403 300 : 348.0586379170418 400 : 467.41283309459686 500 : 587.765111386776 600 : 704.4739812612534 700 : 820.5711499452591 800 : 936.3104489445686 900 : 1053.842625796795 1000 : 1170.2942896485329 1100 : 1284.0791102051735 1200 : 1397.992063820362 1300 : 1513.4232644438744 1400 : 1628.5628224611282 1500 : 1743.8629182577133 1600 : 1857.6377017498016 \n",
      "Train loss: 1971.0829531550407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [42:13<42:13, 1266.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  288.9850385785103\n",
      "Val Acc:  0.5878891885265997\n",
      "Unfreezing layers\n",
      "Epoch:  3 ----------------------------------\n",
      "1700  : 0 : 0.9096465110778809 100 : 95.97625014185905 200 : 177.66485515236855 300 : 252.034624427557 400 : 325.9691686630249 500 : 396.1013750731945 600 : 469.35477340221405 700 : 538.5543161034584 800 : 608.3447819948196 900 : 679.7245073318481 1000 : 745.9303385019302 1100 : 815.500756919384 1200 : 884.1310867369175 1300 : 950.240194439888 1400 : 1017.9852719604969 1500 : 1086.087211072445 1600 : 1150.698146611452 \n",
      "Train loss: 1214.8270719349384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [1:37:41<36:47, 2207.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  162.9590938091278\n",
      "Val Acc:  0.7623191958813434\n",
      "Unfreezing layers\n",
      "Epoch:  4 ----------------------------------\n",
      "1700  : 0 : 0.49026045203208923 100 : 59.17432424426079 200 : 120.14272654056549 300 : 177.7812306135893 400 : 238.1061608940363 500 : 297.23331908881664 600 : 357.74955417215824 700 : 416.6139647513628 800 : 479.3064527362585 900 : 538.2805634588003 1000 : 597.270447358489 1100 : 655.2973441332579 1200 : 715.798188611865 1300 : 776.0568182766438 1400 : 839.7777493596077 1500 : 900.3527182936668 1600 : 959.1219072341919 \n",
      "Train loss: 1012.5034868568182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [2:33:02<00:00, 2295.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:  160.6129099279642\n",
      "Val Acc:  0.7733513115959794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "num_epochs = 4\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    if epoch >= 2:\n",
    "        print(\"Unfreezing layers\")\n",
    "        optimizer = torch.optim.AdamW(bert.parameters(), lr=1e-5)\n",
    "        bert.bert.requires_grad_(True)\n",
    "        bert.classifier.requires_grad_(True)\n",
    "        \n",
    "    print(\"Epoch: \",(epoch + 1), \"----------------------------------\")\n",
    "    bert.train()\n",
    "    total_loss_train = 0\n",
    "    print(len((train_loader)),\" :\", end=\" \")\n",
    "\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = bert(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "        \n",
    "        pred = outputs.logits\n",
    "        loss = loss_fn(pred, batch['labels'])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_train += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print(i, \":\", total_loss_train, end=\" \")\n",
    "    print(f\"\\nTrain loss: {total_loss_train}\")\n",
    "\n",
    "    bert.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss_val = 0\n",
    "    for i, batch in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            outputs = bert(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fn(logits, batch['labels'])\n",
    "        total_loss_val += loss.item()\n",
    "        \n",
    "        correct += (logits.argmax(1) == batch['labels']).sum().item()\n",
    "        total += (logits.argmax(1) == logits.argmax(1)).sum().item()\n",
    "    print(f\"Val loss: \",total_loss_val)\n",
    "    print(f\"Val Acc: \",correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('bert/bert.pkl','wb') as f:\n",
    "    pickle.dump(bert,f)\n",
    "with open('bert/tokenizer.pkl','wb') as f:\n",
    "    pickle.dump(AutoTokenizer.from_pretrained(\"bert-base-cased\"),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:07,  1.62it/s]"
     ]
    }
   ],
   "source": [
    "def predict(bert, loader):\n",
    "    Y = []\n",
    "    Y_pred = []\n",
    "    for i, batch in tqdm(enumerate(loader)):\n",
    "        with torch.no_grad():\n",
    "            Y_pred += bert.to(device)(input_ids = batch['input_ids'], attention_mask = batch['attention_mask']).logits.argmax(1).to('cpu')\n",
    "            Y += batch['labels'].to('cpu')\n",
    "        break\n",
    "    return Y,Y_pred\n",
    "\n",
    "\n",
    "valDataSet = pd.read_csv('../Kaggle-dataset/pre-processed/valDataset.csv',encoding=\"latin1\")\n",
    "testDataSet = pd.read_csv('../Kaggle-dataset/pre-processed/testDataset.csv',encoding=\"latin1\")\n",
    "            \n",
    "Y_val, Y_val_pred = predict(bert, val_loader)\n",
    "Y_test, Y_test_pred = predict(bert, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val\n",
      "\tAcuracia:  0.39801421917136554\n",
      "\tF1:  [0.2294068  0.0224     0.0438247  0.10458284 0.5772168 ]\n",
      "Test\n",
      "\tAcuracia:  0.39650735294117645\n",
      "\tF1:  [0.18214286 0.03296703 0.07328244 0.11945117 0.57944218]\n"
     ]
    }
   ],
   "source": [
    "acc_val = sklearn.metrics.accuracy_score(Y_val, Y_val_pred)\n",
    "f1_val = sklearn.metrics.f1_score(Y_val, Y_val_pred, average=None)\n",
    "acc_test = sklearn.metrics.accuracy_score(Y_test, Y_test_pred)\n",
    "f1_test = sklearn.metrics.f1_score(Y_test, Y_test_pred, average=None)\n",
    "\n",
    "print(\"Val\")\n",
    "print(\"\\tAcuracia: \", acc_val)\n",
    "print(\"\\tF1: \", f1_val)\n",
    "\n",
    "print(\"Test\")\n",
    "print(\"\\tAcuracia: \", acc_test)\n",
    "print(\"\\tF1: \", f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example = 33\n",
    "# print(testDataSet.iloc[example][\"textFull\"])\n",
    "# print( \"Original: \", Y_test[example])\n",
    "# print(\"Predict: \", Y_test_pred[example])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npl-esp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
